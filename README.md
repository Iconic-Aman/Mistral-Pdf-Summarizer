# PDF Summarization using Fine-Tuned Mistral-7B (Unsloth)

This project summarizes lengthy PDF documents using a fine-tuned [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) model with Unsloth for efficient inference on long inputs.

## ğŸ” Features

- Summarizes entire PDFs (even 300-400 pages) by chunking and compressing text.
- Uses Unsloth's optimized model loading for faster generation on 4-bit quantized models.
- Handles large documents by breaking text into manageable chunks.
- Final summary is refined using a second-pass summarization.
- User-friendly Gradio interface for interaction.

## ğŸ“ Project Structure

```your_project/
project_name/
â”‚
â”œâ”€â”€ input_pdfs/                  # Directory for storing input PDF files
â”‚
â”œâ”€â”€ output_image/                # Directory for storing output images
â”‚
â”œâ”€â”€ .gitignore                   # Git ignore file for excluding unnecessary files from version control
â”‚
â”œâ”€â”€ main.ipynb                   # Main Jupyter notebook for Model testing and gradio interface
â”‚
â”œâ”€â”€ mistral_fine_tuning_unsloth.ipynb  # Jupyter notebook for fine-tuning the Mistral model
â”‚
â”œâ”€â”€ README.md                    # Project documentation (explanation of project, setup, and usage)
â”‚
â”œâ”€â”€ requirements.txt             # List of Python dependencies required for the project
â”‚
â”œâ”€â”€ thumbnail.webp             # thumbnail for linkedin video profile

```

## ğŸ§  Model Used

- **Model ID:** `aman012/mistral-7b-instruct-v0.3-bnb-4bit-200`
- **Framework:** Unsloth + HuggingFace Transformers
- **Token Limit during Inference:** 2048 tokens (input must be chunked if larger)

## ğŸš€ How It Works

1. Upload a PDF.
2. Text is extracted and split into ~1500 token chunks.
3. Each chunk is summarized.
4. All summaries are joined and re-summarized for a concise output.

## âš™ï¸ Requirements

Install dependencies:

```bash
pip install -r requirements.txt
```

## ğŸ–¥ï¸ Gradio Web App

```
gradio main.py
```

âš ï¸ Note: This Sace uses a lightweight model due to Hugging Faceâ€™s CPU limitations. The actual fine-tuned 4-bit LLM model is available [here](https://huggingface.co/aman012/mistral-7b-instruct-v0.3-bnb-4bit-200), and runs best in GPU environments like Colab or locally with a compatible GPU setup.

## ğŸ™‹â€â™‚ï¸ Author

* **Aman Kumar Srivastava**

## ğŸ“œ License

MIT License
